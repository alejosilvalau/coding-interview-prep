# Design AlgoExpert
## Gathering System Requirements
We are building the AlgoExpert user flow. This includes:
- Users landing on the website
- Accessing questions
- Marking questions as complete
- Writing code
- Running code
- Having the user's code saved

The answer will not make focus on the payments or authentication. And it won't go too deep into the code-execution engine.

The platform is built for a global audience. But there is a heavy emphasis on US and India, as it is where most of the users are from.

The system availability doesn't need to be overly optimized, as it is a paid product, there isn't the need for more than 2 or 3 nines. This means that in a whole year, there is **1% to 0.1%** of downtime. Which results in **8 hours and 3 days** of downtime per year.

Latency and throughput is something that the system needs to prioritize, since users will expect a fast response when running the code. And they will have their code saved up on the servers. Apart from the code-execution engine, this is not difficult to implement.

## Coming Up With A Plan
AlgoExpert has a lot of static content. For example, the home page is static with lots of images. On the other hand, there is a lot of dynamic content. For example, the code that users themselves can write.

This means that a robust API backing the UI is a must have. And given that the content generated by users gets stored on the website, there is the need for a robust database backing the API as well.

The entire system can be divided into 3 core components:
- Static UI Content
- Accessing and interacting with questions (This means saving solutions, questions completion status, etc.)
- Ability to run code

The second bullet point will get divided further.

## Static UI Content
There will be public assets such as images and JavaScript bundles stored in a blob store. We can use Amazon S3 or Google Cloud Storage (GCS) for that matter.

Since the audience of the website is global, and the main focus is to have a responsive site, a **Content Delivery Network** (CDN) will be necessary. This is crucial on mobile devices, since the internet connection tend to be slower than on Desktop.

## Main Clusters And Load Balancing
The main 2 regions that visit the AlgoExpert.io website are **U.S. and India**. Therefore, it's a must to have the 2 primary clusters of servers located on those regions.

The system can implement DNS load balancing to route API requests to a cluster closest to the user that's fetching the request. Within a region, they system could have **path-based load balancing** to separate the different services such as:
- Payments
- Authentication
- Code execution

This means that a single request could take different paths, even on the same server, depending on what's the main purpose of that request. And comes especially in handy when running the code execution engine, since it will run different kinds of servers in comparison to the rest of the API.

Each service will have a set of servers, and some round-robin load balancing could be implemented. This will be managed in the **path-based** load balancing layer.

## Static API Content
All the static content on AlgoExpert can be stored on a blob storage. Content such as the list of questions and their solutions can be requested through the API, but it's still static content.

The solution of a blob storage is due to simplicity.

## Caching
Caching needs to be implemented to improve the user experience of the website, by reducing load times. To implement it, we can set up 2 layers of caching for the static API content:
- **Client-Side Caching**: it will make users load questions once per session. Which will reduce the load times on the site and reduce the workload on the backend servers, since it will save 2-3 networks calls per session on average.
- **In-Memory Caching**: This will happen on the servers.
  - We can estimate **100 questions, with 10 different languages and 5KB for each solution** to be 100 * 10 * 5000 bytes = **5MB of total data to keep in memory** on average.
  - This is a perfectly fine size of data to keep in memory on a server.

Since the static API content will suffer from changes every few days, we want those changes to be reflected on production as fast as possible. In order to achieve it, the data can be invalidated, evicted and replaced in the server-side cache every 30 minutes.

## Access Control
In AlgoExpert, this regards to the **questions** content. Users who haven't purchased AlgoExpert won't have  access to individual questions.

To implement this, an internal API can be called when a user makes a request for the static API content. In that way, the system can figure out if the user owns the product before returning the full list of questions.

## User Data Storage
As we want to store user's solutions and completion status, we will need SQL tables for that matter. An SQL table like Postgres or MySQL seems like a good choice since we will need to query this data a lot. We need 2 tables, **question_completion_status** and **user_solutions**.

The **question_completion_status** will have the following schema:
- id: integer, (Primary Key, Auto-Incremented)
- user_id: string, (Can be obtained from auth)
- question_id: string,
- completion_status: enum('pending', 'attempted','solved')

The table could have implemented **Uniqueness Constraint** on the user_id and question_id pair. As well as an index on **user_id** for fast querying.

The **user_solutions** table will have the following schema:
- id: integer, (Primary Key, Auto-Incremented)
- user_id: string, (Can be obtained from auth)
- question_id: string,
- language: string, (The language of the solution)
- solution: string (The user's solution itself)

The table could implement **Uniqueness Constraint** on the user_id, question_id and language pair. As  well as an index on **question_id**. If lots of different languages that gets implemented, then an index on **language** as well is not a bad idea. Since that UI won't need to fetch all the user's solutions at the same time, which could cause problems on slow data connections.

## Storage Performance
The type of data that needs to be stored is:
- Marking questions as complete
- The code written on the coding workspace

These two will issue request to the API to store the data on the database. In the case of the code written, it will have a 1 to 3 seconds of debounce time for performance.

Thanks to the user numbers, which is 10000 users on average at any given point in time, the database won't need to handle more than 1000 writes per second. This is a number that SQL databases can handle.

The system could have 2 major databases, each serving the 2 main regions:
- North America
- India (Which will serve Southeast Asia as well)

We could possibly add another cluster serving Europe exclusively.

## Inter-Region Replication
Since the system has 2 primary databases, they need to be in sync. As users on AlgoExpert are located either on US or India, the data written to one database doesn't needs to immediately be written on the other one.

On the other hand, both servers needs to contain the same data at some point. Since users may travel around the world and hit a different database server.

For this, we will have an **async replication** server between the databases. Every 12 hours, the replication will happen by default. But this time frame can vary depending on the amount of data that needs to be replicated.

## Code Execution
It is a must to implement **rate limiting** on the code execution engine. We can do this by having some **tier-based** rate limiting using a K-V store like **Redis**.

Rate limiting is a almost obligatory to prevent DoS Attacks, which is done by limiting the code-execution API. It will also give a good user experience when running code for normal users.

The number of run per second can be limited to:
1. Once every second
2. 3 times per 5 seconds
3. 5 times per minute.

Which are the different tiers.

Since we want only to have a 1-3 second of delay when running code, special servers (Which are called Workers) needs to be ready to run code at all times. To avoid killing the servers each time code is run, they can be cleaned up to remove generated files as a result of compilation.

The backend servers would then contact the free workers and assign code to be run. To then, get a response back from the them when they finish running the code or if the code timed out. The backend servers would then proceed to return the response back to the UI in the same request.

We can estimate that it takes 1 second for the code to compile (If it needs to) and run for each language. Let's say that users perform 10 to 100 **run-code operations** per second. Given that the platform is estimated to have 10000 users, then we need:
```
10000 Users / 100 Run-Code Operations = 100 Machines
```

Therefore, to satisfy the latency requirements of 1-3 seconds per run code, we would need between 10 - 100 machines. This needs to be scaled up if there is a higher load on the system.

In that regard, the system scales horizontally with the number of users. The system can also scale vertically, having faster CPUs. Which would mean faster **run-code operations**.

To make the system scale automatically, it will have logging and monitoring implemented. This is especially the case for the **run-code operations** for tracking run-code events in different ways:
- Per Language
- Per User
- Per Question
- Average Response Time

Among others. When the demand goes up or down, the system then will automatically scale.

An additional benefit that we have by implementing logging and monitoring is knowing when a malicious actor is engaging on malicious behavior on the code execution engine. For example, trying to shutdown the server by sending lots of requests in a short time-frame.


## System Diagram
![algoexpert-design](./design-algoexpert.png)